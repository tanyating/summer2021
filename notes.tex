\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{amsmath,amssymb}

\title{Summer 2021}
\author{Tanya}
\date{June 2021}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\DeclarePairedDelimiterX{\inp}[2]{\langle}{\rangle}{#1, #2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\DeclareMathOperator{\erf}{erf}

\begin{document}

\maketitle


\section{LDA (Linear Discriminant Analysis)}
\begin{itemize}
    \item Notations: $\mathbf{x}$ as input features, $l$ as output label
\end{itemize}

\subsection{Decision Boundary with Prior}

\begin{enumerate}
    \item Model multivariate Gaussian PDF for each class $k \in \{1,..,K\}$:\\
    $p(\mathbf{x}|l=k) = \frac{1}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\mathbf{x}-\mathbf{\mu_k})}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu_k})}}$, $\mathbf{x} \in \R^d$\\
    given $\Sigma$ is the (within-class) covariance matrix and $\mathbf{\mu_k} = \frac{\sum_{i=1}^{i=N_k} \mathbf{x}_i}{N_k}$ from N training data $\mathbf{x}_i$ with labels. 
    
    \item Apply Bayesian Theorem to get the joint PDF: $p(\mathbf{x}, l=k) = p(\mathbf{x}|l=k) \cdot p(l=k)$, given the prior $p(l=k) = \pi_k$. (Posterior probability is the joint conditioned on the data)
    
    \item Assume two classes with $\mathbf{\mu}_1 = \mathbf{0}$ (no signal) and $\mathbf{\mu}_2 = \mathbf{a}$ (signal $\mathbf{a} \in \R^d$) with same covariance $\Sigma = \sigma^2 I$ (given noise $\sigma$ for each independent feature). Find decision boundary (hyperplane) between the 2 classes such that $p(\mathbf{x}|l=1)\cdot p(l=1) = p(\mathbf{x}|l=2)\cdot p(l=2)$. (Note: $\pi_1 = p(\mathbf{0})$, $\pi_2 = p(\mathbf{a})$).
    This is called the MAP (max a posterior) decision rule. 
    \begin{itemize}
    \item Proposition: The MAP decision boundary is 
    \begin{equation}
         \frac{\inp {\mathbf{x}}{\mathbf{a}}}{\norm {\mathbf{a}}} = \frac{\norm {\mathbf{a}}}{2} + \frac{\sigma^2}{\norm {\mathbf{a}}}\log{\frac{\pi_1}{\pi_2}}
    \end{equation}
    Proof: \\\\
    $\frac{\pi_1}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\mathbf{x}-\mathbf{\mu}_1)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_1)}} = \frac{\pi_2}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\mathbf{x}-\mathbf{\mu}_2)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_2)}}$ \\\\
    $2\log{\frac{\pi_1}{\pi_2}} = {(\mathbf{x}-\mathbf{\mu}_1)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_1)} - {(\mathbf{x}-\mathbf{\mu}_2)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_2)}$\\\\
    $2\log{\frac{\pi_1}{\pi_2}} = \frac{1}{\sigma^2}{(\mathbf{x}-\mathbf{\mu}_1)}^T I^{-1}{(\mathbf{x}-\mathbf{\mu}_1)} - \frac{1}{\sigma^2}{(\mathbf{x}-\mathbf{\mu}_2)}^T I^{-1}{(\mathbf{x}-\mathbf{\mu}_2)}$ (replace $\Sigma$ with $\sigma^2 I$)\\\\
    $\mathbf{x}^T\mathbf{x} - {(\mathbf{x}-\mathbf{a})}^T(\mathbf{x}-\mathbf{a}) = 2\sigma^2\log{\frac{\pi_1}{\pi_2}}$\\\\
    $\frac{\mathbf{x}^T\mathbf{a}}{\norm {\mathbf{a} }} = \frac{\inp {\mathbf{x}} {\mathbf{a}}}{\norm {\mathbf{a} }} = \frac{\norm {\mathbf{a} }}{2} + \frac{\sigma^2}{\norm {\mathbf{a} }}\log{\frac{\pi_1}{\pi_2}}$
    
    
        \item If unbiased ie. $\pi_1=\pi_2$, then $\frac{\mathbf{x}^T\mathbf{a}}{\norm {\mathbf{a} }} = \frac{\norm {\mathbf{a} }}{2}$ which geometrically means the decision hyperplance is in the middle between $\mathbf{0}$ and $\mathbf{a}$ (Projected onto 1-D, the decision point is $x=\frac{\norm {\mathbf{a} }}{2}$).
        \item If biased with no signal ie. $\pi_1>\pi_2$, then $\log{\frac{\pi_1}{\pi_2}}>0$, which geometrically means the decision hyperplance shifts towards $\mathbf{a}$.
        \item If biased with signal ie. $\pi_1<\pi_2$, then $\log{\frac{\pi_1}{\pi_2}}<0$, which geometrically means the decision hyperplance shifts towards $\mathbf{0}$.
    \end{itemize}
    
    \item Prediction (Detection) for some $\mathbf{x} \in \R^d$ based on decision hyperplane:\\
    If $\inp {\mathbf{x}} {\mathbf{a}} \leq \frac{\norm {\mathbf{a} }_2^2}{2} + \sigma^2\log{\frac{\pi_1}{\pi_2}}$, predict $l=1$ (no signal). Else, predict $l=2$ (signal).
    
\end{enumerate}

\subsection{False Positive and False Negative Rates in/projected-onto 1-D}
    
    \begin{enumerate}
    \item Model Gaussian PDF for each class $k \in \{1,..,K\}$:
    $p(x|l=k) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{{(x-\mu_k)}^2}{2\sigma^2}}$ ($x \in \R$),\\
    given $\sigma^2=\frac{\sum_{i=1}^{i=N} {(x_i-\mu_i)}^2}{N}$ and $\mu_k = \frac{\sum_{i=1}^{i=N_k} x_i}{N_k}$ from N training data $x_i$ with labels. 
    \item Assume two classes with $\mu_1 = 0$ (no signal) and $\mu_2 = a$ (signal) with same noise $\sigma$. Find decision boundary between the 2 classes to be $x=\frac{a}{2}$ where $p(x=\frac{a}{2}|l=1) = p(x=\frac{a}{2}|l=2)$ (Maximum likelihood classifier).
    \item Detection (prediction) of some $x$: \\
    Maximize likelihood: if $p(x|l=1) \ge p(x|l=2)$, predict $l=1$ (no signal), else $l=2$ (signal).\\
    Decision boundary (geometric): if $x \leq \frac{a}{2}$, predict $l=1$ (no signal), else $l=2$ (signal).
    \item Error Matrix:
    
    \begin{tabular}{l|l|l}
    Truth/Detection&No Signal&Signal\\\hline
    No Signal&Correct&False Positive\\\hline
    Signal&False Negative&Correct\\
    \end{tabular}
    \item False Positive rate: the area of the right tail of label-1 (no signal) Gaussian PDF beyond decision boundary $x=b$.
    \begin{equation}
        \rho_{fp} = \int_{\frac{b}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(\frac{b}{\sqrt{2}\sigma})
    \label{eq:1}
    \end{equation}
    
    False Negative rate: the area of the left tail of label-2 (signal) Gaussian PDF below decision boundary $x=b$.
    
    \begin{equation}
        \rho_{fn} = \int_{-\infty}^{\frac{b-a}{\sigma}} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \int_{-\frac{b-a}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(-\frac{b-a}{\sqrt{2}\sigma})
    \label{eq:2}
    \end{equation}
    
    
    \item Since there is no prior (unbiased) and decision boundary $b=\frac{a}{2}$, False Negative rate $=$ False Positive rate $= \frac{1}{2} - \frac{1}{2} \erf(\frac{a}{2\sqrt{2}\sigma})$.
\end{enumerate}

\subsection{Decision Boundary with Prior and Penalty Matrix}

\begin{enumerate}
    \item Determine decision boundary (in any dimension d, $\mathbf{x} \in \R^d$) with prior and penalty matrix. (two classes with $\mathbf{\mu}_1 = \mathbf{0}$ (no signal) and $\mathbf{\mu}_2 = \mathbf{a}$ (signal))
    \item Assume prior (number of occurrences) for no signal is $\pi_1$, and prior for signal is $\pi_2$ ($\pi_2 = 1-\pi_1$). 
    \item Penalty/Cost Matrix (corresponding to the Error Matrix):
    
    \begin{tabular}{l|l|l}
    Truth/Detection&No Signal&Signal\\\hline
    No Signal&0&F (fp)\\\hline
    Signal&M (fn)&0\\
    \end{tabular}
    \item Decision Rule: If $\inp {\mathbf{x}} {\frac{\mathbf{a} }{\norm {\mathbf{a} }}} \le b$, predict $l=1$ (no signal). Else, predict $l=2$ (signal). $b$ is the decision point.
    \item False positive rate and false negative rate given decision point $b$:
    \begin{equation}
        \rho_{fp} = \alpha(b) = \int_{\frac{b}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(\frac{b}{\sqrt{2}\sigma})
    \end{equation}
    
    \begin{equation}
        \rho_{fn}= \beta(b) = \int_{-\infty}^{\frac{b-a}{\sigma}} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \int_{-\frac{b-a}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(-\frac{b-\norm {\mathbf{a} }}{\sqrt{2}\sigma})
    \end{equation}
    
    \item Expected cost function: $\mathbf{E}[J] = \pi_1\alpha(b)F+(1-\pi_1)\beta(b)M$
    \item Minimize cost function w.r.t. $b$, assuming $M=1$.\\
    $\min_b \mathbf{E}[J]$, Take $\nabla \mathbf{E}[J] = 0$\\\\
    $\pi_1 F(-\frac{1}{2}\cdot\frac{2}{\sqrt{\pi}}\cdot e^{-\frac{b^2}{2\sigma^2}}\cdot \frac{1}{\sqrt{2}\sigma}) +
    (1-\pi_1)(-\frac{1}{2}\cdot\frac{2}{\sqrt{\pi}}\cdot e^{-\frac{{(b-\norm {\mathbf{a} })}^2}{2\sigma^2}}\cdot -\frac{1}{\sqrt{2}\sigma})=0$\\\\
    $\frac{1-\pi_1}{\sqrt{2\pi}\sigma} e^{-\frac{{(b-\norm {\mathbf{a} })}^2}{2\sigma^2}} = \frac{\pi_1 F}{\sqrt{2\pi}\sigma} e^{-\frac{b^2}{2\sigma^2}}$\\\\
    $\log{(1-\pi_1)}-\frac{{(b-\norm {\mathbf{a} })}^2}{2\sigma^2} =
    \log{(\pi_1F)}-\frac{b^2}{2\sigma^2}$\\\\
    $\frac{b^2-{(b-\norm {\mathbf{a} })}^2}{2\sigma^2} = log{(\frac{\pi_1F}{1-\pi_1})}$\\\\
    $2\norm {\mathbf{a} } b - {\norm {\mathbf{a} }}^2 = 2\sigma^2log{(\frac{\pi_1F}{1-\pi_1})}$\\\\
    \begin{equation}
        b = \frac{\sigma^2log{(\frac{\pi_1F}{1-\pi_1})}}{\norm {\mathbf{a} }} + \frac{\norm {\mathbf{a} }}{2}
    \end{equation}
    \item Relate to the MAP decision boundary: if $M=F=1$, ie. if we minimize both false positive rate and false negative rate equally (same penalty), then the decision boundary $\inp {\mathbf{x}} {\frac{\mathbf{a} }{\norm {\mathbf{a} }}} = b = \frac{\sigma^2log{(\frac{\pi_1}{1-\pi_1})}}{\norm {\mathbf{a} }} + \frac{\norm {\mathbf{a} }}{2}$ is the same as the MAP decision boundary (which only depends on the prior). 
    
\end{enumerate}


\section{Discrete Pixel Problem}

\subsection{Set-up}
\begin{itemize}
    \item Molecule: Assume there's some molecule $m \in \R^{p \times q}$ ($p\ge q$) in 2-D, with 1-D projection onto $N$ number of pixels. The molecule $m$ has $N-p+1$ translations $t \in \{0,1,2,...,N-p\}$, and 4 rotations $R \in \{0, \frac{\pi}{2}, \pi, \frac{3\pi}{2}\}$ at every discrete position. For each instance (1-D projection) $\mathbf{y} \in \R^N$, each component $y^i$ of $\mathbf{y}$ represents the sum along the ith column. In total, there are $N_c = 4\times(N-p+1)$ configurations (set of translations and rotations) for signals (ie. molecule detected). 
    \begin{itemize}
        \item We can construct random molecule of size $p\times q$ by drawing random entries from the standard uniform distribution $U(0,1)$ for each $m_{ij}$. 
        \item Normalize the molecule: for the random entries drawn from the standard uniform distribution, we can normalize it to mean 0 and unit variance.
    \end{itemize}
    \item Template: Given a molecule $m$, we can generate a set of templates $A \in \R^{N_c \times N}$ such that each row $A(k,:) \in \R^N = a_{t=i, R=(j-1)\frac{\pi}{2}}$ represents a projected signal with certain translation ($i \in {0,..,N-p}$) and rotation ($j \in {1,..,4}$). (We index each unique pair of translation $i$ and rotation $j$ as a positive class $k=i+4\cdot j$, and $0$ represents the no molecule case.)
    \item Random (input) data with noise:
    \begin{enumerate}
        \item Definition of an input vector $\mathbf{y}$: $\mathbf{y} =  \mathbf{a}_{t_0,R_0} + \epsilon_\sigma$ ($\mathbf{y}, \mathbf{a}_{t_0,R_0}, \epsilon_\sigma \in \R^N$ and $ \mathbf{a}_{t_0,R_0}$ as some signal, $\epsilon_\sigma$ as iid noise $N(0, \sigma^2)$)
        \item Extend to $M$ instances: an input matrix $\mathbf{Y} = [\mathbf{y_1}^T; \mathbf{y_2}^T; ...; \mathbf{y_M}^T] \in \R^{M\times N}$
    \end{enumerate}
    \end{itemize}
    
    \subsection{Detection/Prediction for random data}
    \begin{itemize}
        \item Given an input matrix $\mathbf{Y} = [\mathbf{y_1}^T; \mathbf{y_2}^T; ...; \mathbf{y_M}^T] \in \R^{M\times N}$, for each $\mathbf{y_i}$ find most likely $t$, $R$ w.r.t $\mathbf{y_i}$ ($N_c+1$ possible outcomes with 1 for no molecule). (Assume prior $p_0=0.5$ such that half of the instances are just noise, and the rest of instances with signal are evenly distributed into different sets of translation and rotation (configurations) with probability $\frac{1-p_0}{N_c}$)
        \item Methods of detection/prediction (for each $\mathbf{y_i}$, $i \in {1,...,M}$)
        \begin{enumerate}
        \item $\min_{t,R} d_{1}(\by_i,\mathbf{a}_{t,R}) = \min_{t, R} \norm {\by_i- \mathbf{a}_{t,R}}^2$
        \begin{itemize}
            \item This is the same as modeling each configuration of signal($\mathbf{a}_{t,R}$) and the no signal (origin) as a multivariate Gaussian distribution with same noise $\sigma$ (assuming prior for all $N_c+1$ outcomes is equal).
            \item Theoretical Bounds for false positive/negative rates: For each $\mathbf{a}_{t,R}$, we can construct a decision hyperplane between $\mathbf{a}_{t,R}$ and $\mathbf{0}$. Choose the nearest hyperplane $\widetilde{b} = \frac{a_{min}}{2}$ between the origin and $\mathbf{a}_{min} = \min{\norm { \mathbf{a}_{t,R}}}$, we can get a lower bound for false positive rate as $\frac{1}{2} - \frac{1}{2} \erf(\frac{\widetilde{b}}{\sqrt{2}\sigma})$, and a upper bound as $(\frac{1}{2} - \frac{1}{2} \erf(\frac{\widetilde{b}}{\sqrt{2}\sigma}))\cdot N_c$.
        \end{itemize}
        \item $\max_{t,R} d_{2}(\hat{\by}_i, \hat{\ba}_{t,R}) = \max_{t,R} \inp {\hat{\by}_i} {\hat{\ba}_{t,R}}$, where $\hat{v} = \frac{1}{c}(\mathbf{v} - v_m)$ and $v_m = \frac{\sum_{i=1}^N v_i}{N}$, $c = \norm {\mathbf{v}- v_m}$
        \begin{itemize}
            \item Normalize each $\by_i$ and $\ba_{t,R}$ to get corresponding $\hat{\by}_i$ and $\hat{\ba}_{t,R}$ with mean 0 and unit variance.
            \item Since the noise center (origin) $\mathbf{0}$ cannot be "normalized" and always $d_{2}(\hat{\by}_i, \mathbf{0})=0$, the false positive rate could be really high (almost $1$) by maximizing $d_2$.
            \item To make this method reasonably separate positive (signal) class and negative (noise) class, we add a threshold $\tau$ to make the decision. If $\max_{t,R} d_{2}(\hat{\by}_i, \hat{\ba}_{t,R}) = \max_{t,R} \inp {\hat{\by}_i} {\hat{\ba}_{t,R}} \ge \tau$, we choose the corresponding maximizer $\widetilde{t}$, $\widetilde{R}$ as the predicted positive label; else, we assign label $0$ (no molecule).
            \item We can choose the threshold $\tau$ based on histogram study or empirical error rates for the prediction.
        \end{itemize}
        \item Let $f(t,\by_i) = \max_R \inp {\hat{\by}_i} {\hat{\ba}_{t,R}}$. Pick a threshold $\tau$ based on false positive/negative rates (penalty).\\
        If $f(\widetilde{t},\by_i) > \tau$ and $\widetilde{R} = \argmax_R \inp {\hat{\by}_i} {\hat{\ba}_{\widetilde{t},R}}$, then $\widetilde{t}$, $\widetilde{R}$ is a candidate.
        \item Let $g(t,\by_i) = {mean}_R \inp {\hat{\by}_i} {\hat{\ba}_{t,R}}$, $h(t,\by_i) = {variance}_R  \inp {\hat{\by}_i} {\hat{\ba}_{t,R}}e$, and $j(t,\by_i) = \frac{f(t,\by_i)-g(t,\by_i)}{\sqrt{h(t,\by_i)}}$. \\
        If $j(\widetilde{t},\by_i) > \tau$ and $\widetilde{R} = \argmax_R \inp {\hat{\by}_i} {\hat{\ba}_{\widetilde{t},R}}$, then $\widetilde{t}$, $\widetilde{R}$ is a candidate.
    \end{enumerate}
    
    \end{itemize}
    
    \subsection{Detection/Prediction Error Measurements}
    
    \subsubsection{Confusion/Error matrix}
    Given known true labels ($\mathbf{L_T} \in \R^{M\times 1}$) and predicted labels ($\mathbf{L_P} \in \R^{M\times 1}$), we can construct a confusion (error) matrix $C$ ($N_c+1$ number of rows and columns):
    \\\\
    
    \begin{tabular}{l|l|l|l|l}
        Truth/Detection&$0$&$1$&...&$N_c$\\\hline
        $0$ (No Signal)&Correct& &...& \\\hline
        $1$ ($t=0,R=0$)& &Correct&...& \\\hline
        ...& & & & \\\hline
        $N_c$ ($t=N-p,R=\frac{3\pi}{2}$)& & &...&Correct\\\hline
    \end{tabular}
        
        \begin{enumerate}
            \item For each instance $\by_k$ ($k \in \{1,...,M\}$), there's a corresponding true label $\mathbf{L_T}[k] \in {\{0,..,N_c\}} = i \cdot 4 + j$, indicating a unique pair of translation $i \in {\{0,..,N-p\}}$ and rotation $j \in {\{1,..,4\}}$), and similarly a corresponding predicted label $\mathbf{L_P}[k]$ (detected from one of the above methods). We can add up the number of occurrences to the corresponding entries in the confusion matrix $C$.
            \item $0$ represents the no molecule (noise) case and corresponds to translation $0$ and rotation $0$. (We always put the noise label $0$ as the first class)
            \item Before normalization, each entry corresponds to the number of occurrences based on truth label and detected label ($\sum_{i=1}^{N_c+1}\sum_{j=1}^{N_c+1} C_{ij} = M$).
            \item Normalization: 
            \begin{itemize}
                \item Divide each entry by total number of instances ($\sum_{i=1}^{N_c+1}\sum_{j=1}^{N_c+1} C_{ij} = 1$).
                \item Normalize along each row such that for each row $i \in \{1,...,Nc+1\}$, $\sum_{j=1}^{N_c+1} C_{ij} = 1$. (This means each ith row gives the fractions of classifications given to the true label $i$.)
            \end{itemize}
            \item Perfect detection corresponds to a diagonal matrix (if normalized, $C_{11}=p_0$, $C_{ii}=\frac{1-p_0}{Nc}$ for $i>1$).
            \item If we use the threshold prediction/detection for each translation instead of min/max "distance" over all pairs of translation and rotation, each instance may get multiple predicted/detected labels. In this case, we consider each prediction of each instance as an occurrence to add in the error matrix. 
        \end{enumerate}
        
    \subsubsection{Translation-wise Confusion Matrix}
     We can also construct a Translation-wise Confusion Matrix $C_t$ ($N_t+1=N-p+2$ number of rows and columns):
     \\\\
        \begin{tabular}{l|l|l|l|l}
        Truth/Detection&$0$&$1$&...&$N_t=N-p$\\\hline
        $0$ (No Signal)&Correct& &...& \\\hline
        $1$ ($t=0$)& &Correct&...& \\\hline
        ...& & & & \\\hline
        $N_t$ ($t=N-p$)& & &...&Correct\\\hline
        \end{tabular}
        
        \begin{enumerate}
            \item For each instance $\by_k$ ($k \in \{1,...,M\}$), we only care about the true translation label $\mathbf{\tilde{L}_T}[k] = \floor{\frac{\mathbf{L_T}[k]}{4}} = i \in {\{0,..,N-p\}}$ and the predicted label $\mathbf{\tilde{L}_P}[k]$. To keep $i=0$ represents no molecule, we re-index the translation such that $i \in {\{1,..,N-p+1\}}$. So for the error matrix, truth/detection each have $N-p+2$ elements ${\{0,..,N-p+1\}}$.
            
            \item The intersection of the 1st column and 1st row $C_{t}[0,0]=tn$ is the true negative rate $c$ (noise detected as noise). 
                \item Regardless of 1st entry, the rest of the first row ($C_{t}[0,i], i \in \{1,...,N-p+1\}$) is roughly the same value $a$, which indicates some false positive rate for the noise (noise detected as signal/molecule).
                \item Regardless of 1st entry, the rest of the first column ($C_{t}[i,0], i \in \{1,...,N-p+1\}$) is roughly the same value $b$, which indicates some false negative rate for the noise (signal/molecule detected as noise).
                \item Regardless of the 1st row and 1st column, the rest of the matrix ($C_{t}[i,j], i,j \in \{1,...,N-p+1\}$) is a Toeplitz matrix. Focus on any row $i$, ie. true label of translation is $i$: 
                \begin{enumerate}
                    \item $d=C[i,i]$ is the true positive rate for the molecule at translation $i$. 
                    \item In an "overlapping" neighborhood of $C[i,i]$, ie. $\{C[i,j]: \norm {j-i } < k\}$ where $k=\max{(p,q)}$, when the predicted signal at the predicted translation $j$ overlaps/touches the true signal at translation $i$, there are some mis-classifying rates $e_{z}=C[i,i+z]$ ("false class rate" dependent of $z=j-i$) for each predicted translation $j \neq i$.
                    \item Outside of the overlapping region, ie. $\{C[i,j]: \norm {j-i } \ge k\}$ where $k=\max{(p,q)}$, there is some similar "noise" rate $F=C[i,j]$ ("false positive rate for molecule", independent of $j$) for every predicted translation $j$.
                \end{enumerate}
            
            \item Since the observation above works for any row $i$ (ignore 1st column and 1st row), we can only focus on the 1st and a "central" row of a single true translation. We pick the true translation $i=\ceil{\frac{N}{2}}$, such that the molecule moves to the middle of the pixels and covers the largest overlapping region. We can recover the full matrix with the first row, first column with all entries the same as the 1st entry of the "central" row, and the rest of entries of other rows can be filled with the overlapping region around the true translation $i$ and noise rates else where. (An example of "reduced" translation-wise error matrix with the "central" row $i=\ceil{\frac{N_t}{2}}$ and molecule of $p=2$ (overlapping region at the ith row is $\{i-1,i+1\}$))
            \\\\
            \begin{tabular}{l|l|l|l|l|l|l|l}
            Truth/Detection&$0$&...& &$i-1$&$i$&$i+1$&...\\\hline
            $0$(No Signal)&$c$&... &$a$&$a$&$a$&$a$& \\\hline
            ...&$b$& & & & & & \\\hline
            $i$&$b$&...&$F$ &$e_{-1}$&$d$&$e_{1}$&$F$\\\hline
            ...&$b$&...& & & & & \\\hline
            
            \end{tabular}
            
            \item The above observation can be extended to translation-rotation error matrix $C$. Since the projection for each rotation is the same at any translation, we only care about the different "error rates" from the first row and 4 more rows indicating a single "central" true translation paired up with 4 true rotations. \\ (ie. For "reduced" $C$, there are rows $[0, 0]^T, [\ceil{\frac{N}{2}}, 1]^T, [\ceil{\frac{N}{2}}, 2]^T, [\ceil{\frac{N}{2}}, 3]^T, [\ceil{\frac{N}{2}}, 4]^T$)
            \begin{enumerate}
                \item Average rate of getting true translation, wrong rotation: Focus on the 4-by-4 square matrix as below (true labels and predicted labels both consist of central translation with 4 rotations), this is the "center" of the overlapped region when the predicted translation is correct. The diagonal values $C_{ii}$ indicates the true positive rate for central translation with each rotation. So we can take the average of the rest of entries except the diagonal values to get the mean rate $g$ of getting true translation, but wrong rotation. (ie. $g = \frac{\sum_{i \neq j} C_{ij}}{4\times 4-4}=\frac{\sum_{i \neq j} C_{ij}}{12}$)
                
                \begin{tabular}{l|l|l|l|l}
                    Truth/Detection&$[\ceil{\frac{N}{2}}, 1]^T$&$[\ceil{\frac{N}{2}}, 2]^T$&$[\ceil{\frac{N}{2}}, 3]^T$&$[\ceil{\frac{N}{2}}, 4]^T$\\\hline
                    $[\ceil{\frac{N}{2}}, 1]^T$ ($t=\ceil{\frac{N}{2}},R=0$)&Correct& &...& \\\hline
                    $[\ceil{\frac{N}{2}}, 2]^T$ ($t=\ceil{\frac{N}{2}},R=\frac{\pi}{2}$)& &Correct&...& \\\hline
                    $[\ceil{\frac{N}{2}}, 3]^T$ ($t=\ceil{\frac{N}{2}},R=\pi$)& & & & \\\hline
                    $[\ceil{\frac{N}{2}}, 4]^T$ ($t=\ceil{\frac{N}{2}},R=\frac{3\pi}{2}$)& & &...&Correct\\\hline
                \end{tabular}
                
                \item Average false positive rate per rotation: Focus on the 1st row of ("reduced") $C$, ignore the 1st entry ($C_{11}$ is detected true noise), each entry of the rest ($C_{1j}$, $j>1$) is some false positive rate when the noise is detected as a molecule with a pair of translation and rotation. Since observation above shows that each translation is the same, the false positive rate for every translation and a rotation should be the same. (ie. the false positive rates along the row should repeat every 4 columns) So we can compute the average false negative rate at rotation $j$ as $h_j = \frac{\sum_{n=0} C_{i, 1+4n + j}}{\frac{N_c}{4}}=\frac{\sum_{n=0} C_{i, 1+4n + j}}{N_t}$.
                
                \begin{tabular}{l|l|l|l|l|l|l|l|l|l}
                    Truth/Detection&$[0, 1]^T$&$[0, 2]^T$&$[0, 3]^T$&$[0, 4]^T$&$[1, 1]^T$&$[1, 2]^T$&$[1, 3]^T$&$[1, 4]^T$&...\\\hline
                    $0$ (No Signal)&$h_1$&$h_2$&$h_3$&$h_4$&$h_1$&$h_2$&$h_3$&$h_4$&... \\\hline
                \end{tabular}
                
                \item Approximate false negative rate per rotation: Focus on the 1st column of ("reduced") $C$, ignore the 1st entry ($C_{11}$ is detected true noise), each entry of the rest ($C_{i1}$, $i>1$) is some false negative rate when a molecule with "central" translation and some rotation is detected as noise. So we can compute the average false negative rate at rotation $j \in \{1,...,4\}$ as $o_j = C_{j+1, 1}$. (View the below vector as a transpose of the desired column)
                
                \begin{tabular}{l|l|l|l|l|l|l|l|l|l}
                    Detection/Truth&$[\ceil{\frac{N}{2}}, 1]^T$&$[\ceil{\frac{N}{2}}, 2]^T$&$[\ceil{\frac{N}{2}}, 3]^T$&$[\ceil{\frac{N}{2}}, 4]^T$\\\hline
                    $0$ (No Signal)&$o_1$&$o_2$&$o_3$&$o_4$\\\hline
                \end{tabular}
                
            \end{enumerate}
            
        \end{enumerate}
    
    
    \subsubsection{Extract Error Rates and Compare}
    Extract the distinct error rates from above confusion matrices (reduced $C$ and $C_t$) and compare across whether or not normalize the molecule, different ratios of molecule size ($\frac{p}{q}$), and different methods to detect.
    
    \begin{enumerate}
        \item Use method 1 ($\min_{t,R} d_{1}(\by_i,\mathbf{a}_{t,R})$), compare across whether or not normalize the molecule and molecule size ($[p,q] \in \{[4,4], [8,2]\}$)
        
        \begin{itemize}
        
            \item General False (positive/negative) rates (mol vs. noise)
            \begin{enumerate}
                \item Un-normalized molecule tends to have lower general $fp$ than normalized molecule, $F$. ie. Normalized molecule are more likely to be mistakenly detected as molecule.
                \item $8\times 2$ molecule tends to have higher general $fp$ than $4\times 4$ molecule. ie. high-ratio molecule are more likely to be detected as noise.
            \end{enumerate}
        
            \item False Positive rates per rotation R (mol vs. noise)
            \begin{enumerate}
                \item For $4\times 4$ square molecule, normalized molecule tends to have higher $fp$ when $R=1,3$ (when un-normalized, $fp$ at $R=1,3$ is close to $R=2,4$). ie. when normalized, square matrix are more likely to detect as noise at fat rotation.
                \item For $8\times 2$ square molecule, un-normalized molecule tends to have higher $fp$ when $R=1,3$
            \end{enumerate}
            
            \item False Negative rates per rotation R (mol vs. noise)
            \begin{enumerate}
                \item For $4\times 4$ square molecule, normalized molecule tends to have higher $fn$ when $R=1,3$ (when un-normalized, $fn$ at $R=1,3$ is close to $R=2,4$). ie. when normalized, noise is more likely to detect as a square molecule at fat rotation.
                \item For $8\times 2$ square molecule, un-normalized molecule tends to have higher $fn$ when $R=1,3$
            \end{enumerate}
        
            \item Mis-classifying rates (mol vs. mol)
            \begin{enumerate}
                \item For every true translation, un-normalized molecule tends to have higher $g$ and lower $e$, $F$ than normalized molecule. ie. Un-normalized molecule are more likely to get true translation.
                \item For every true translation, $8\times 2$ molecule tends to have higher $F$ than $4\times 4$ molecule. ie. high-ratio molecule are more likely to be detected with non-overlapping translation.
            \end{enumerate}
        \end{itemize}
        
    \end{enumerate}

\subsection{Histogram study}
Let $\epsilon_{\sigma}^{i}  \in \mathbb{R}^{N}$, $i=1,2,\ldots M$, denote iid noise vectors with in $\mathbb{R}^{N}$ with mean $0$ and variance $\sigma^2$ as before. (Based on the the approximate distribution of $d(\epsilon_{\sigma}^i, \ba_{t,R})$ and a threshold $\tau$, we can preview the approximate false positive rate by adding up the relative-frequencies for $d>\tau$.)

\begin{enumerate}
\item Plot the histogram of values $d_{1}(\epsilon_{\sigma}^i, \ba_{t,R}), d_{2}(\epsilon_{\sigma}^i, \ba_{t,R})$, $t\in 1,2,\ldots N-p$, $R \in \{ 0,\pi/2, \pi, 3\pi/2 \}$ 
\item For fixed R, plot the histogram values $d_{1}(\epsilon_{\sigma}^i, \ba_{t,R}), d_{2}(\epsilon_{\sigma}^i, \ba_{t,R})$, $t\in 1,2,\ldots N-p$. Also plot the histogram values of $f(t,\epsilon_{\sigma}^i)$, and $j(t,\epsilon_{\sigma}^i)$.
\end{enumerate}




\end{document}