\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{amsmath,amssymb}

\title{Summer 2021}
\author{Tanya}
\date{June 2021}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\ba}{\boldsymbol{a}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\DeclareMathOperator{\erf}{erf}

\begin{document}

\maketitle

\section{Introduction}
This project is about... 


\section{Notes}

\subsection{\date{Jun 10th}}
LDA (Linear Discriminant Analysis, $\mathbf{x}$ as input features, $l$ as output label): 
\begin{itemize}
    \item 1-D ($x \in \mathbf{R}$) false positive and false negative rates (no prior or unbiased):
\begin{enumerate}
    \item Model Gaussian PDF for each class $k \in \{1,..,K\}$:
    $p(x|l=k) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{{(x-\mu_k)}^2}{2\sigma^2}}$,\\
    given $\sigma^2=\frac{\sum_{i=1}^{i=N} {(x_i-\mu_i)}^2}{N}$ and $\mu_k = \frac{\sum_{i=1}^{i=N_k} x_i}{N_k}$ from N training data $x_i$ with labels. 
    \item Assume two classes with $\mu_1 = 0$ (no signal) and $\mu_2 = a$ (signal) with same noise $\sigma$. Find decision boundary between the 2 classes to be $x=\frac{a}{2}$ where $p(x=\frac{a}{2}|l=1) = p(x=\frac{a}{2}|l=2)$ (Maximum likelihood classifier).
    \item Detection (prediction) of some $x$: \\
    Maximize likelihood: if $p(x|l=1) \ge p(x|l=2)$, predict $l=1$ (no signal), else $l=2$ (signal).\\
    Decision boundary (geometric): if $x \leq \frac{a}{2}$, predict $l=1$ (no signal), else $l=2$ (signal).
    \item Error Matrix:
    
    \begin{tabular}{l|l|l}
    Truth/Detection&No Signal&Signal\\\hline
    No Signal&Correct&False Positive\\\hline
    Signal&False Negative&Correct\\
    \end{tabular}
    \item False Positive rate: the area of the right tail of label-1 (no signal) Gaussian PDF beyond decision boundary $x=b$.
    \begin{equation}
        fp = \int_{\frac{b}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(\frac{b}{\sqrt{2}\sigma})
    \label{eq:1}
    \end{equation}
    
    False Negative rate: the area of the left tail of label-2 (signal) Gaussian PDF below decision boundary $x=b$.
    
    \begin{equation}
        fn = \int_{-\infty}^{\frac{b-a}{\sigma}} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \int_{-\frac{b-a}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(-\frac{b-a}{\sqrt{2}\sigma})
    \label{eq:2}
    \end{equation}
    
    
    \item Since there is no prior (unbiased) and decision boundary $b=\frac{a}{2}$, False Negative rate $=$ False Positive rate $= \frac{1}{2} - \frac{1}{2} \erf(\frac{a}{2\sqrt{2}\sigma})$.
\end{enumerate}

\item Determine decision boundary (geometrically in any dimension d, $\mathbf{x} \in \mathbf{R}^d$) with prior:
\begin{enumerate}
    \item Model multivariate Gaussian PDF for each class $k \in \{1,..,K\}$:\\
    $p(\mathbf{x}|l=k) = \frac{1}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\mathbf{x}-\mathbf{\mu_k})}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu_k})}}$,\\
    given $\Sigma$ is the (within-class) covariance matrix and $\mathbf{\mu_k} = \frac{\sum_{i=1}^{i=N_k} \mathbf{x}_i}{N_k}$ from N training data $\mathbf{x}_i$ with labels. 
    \item Apply Bayesian Theorem to get posterior probability: $p(\mathbf{x}, l=k) = p(\mathbf{x}|l=k) \cdot p(l=k)$, given the prior $p(l=k) = \pi_k$.
    \item Assume two classes with $\mathbf{\mu}_1 = \mathbf{0}$ (no signal) and $\mathbf{\mu}_2 = \mathbf{a}$ (signal $\mathbf{a} \in \mathbf{R}^d$) with same covariance $\Sigma = \sigma^2 I$ (given noise $\sigma$ for each independent feature). Find decision boundary (hyperplane) between the 2 classes such that $p(\mathbf{x}|l=1)\cdot p(l=1) = p(\mathbf{x}|l=2)\cdot p(l=2)$. (Note: $\pi_1 = p(\mathbf{0})$, $\pi_2 = p(\mathbf{a})$)\\
   
    $\frac{\pi_1}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\mathbf{x}-\mathbf{\mu}_1)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_1)}} = \frac{\pi_2}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\mathbf{x}-\mathbf{\mu}_2)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_2)}}$ \\\\
    $2\log{\frac{\pi_1}{\pi_2}} = {(\mathbf{x}-\mathbf{\mu}_1)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_1)} - {(\mathbf{x}-\mathbf{\mu}_2)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_2)}$\\\\
    $2\log{\frac{\pi_1}{\pi_2}} = \frac{1}{\sigma^2}{(\mathbf{x}-\mathbf{\mu}_1)}^T I^{-1}{(\mathbf{x}-\mathbf{\mu}_1)} - \frac{1}{\sigma^2}{(\mathbf{x}-\mathbf{\mu}_2)}^T I^{-1}{(\mathbf{x}-\mathbf{\mu}_2)}$ (replace $\Sigma$ with $\sigma^2 I$)\\\\
    $\mathbf{x}^T\mathbf{x} - {(\mathbf{x}-\mathbf{a})}^T(\mathbf{x}-\mathbf{a}) = 2\sigma^2\log{\frac{\pi_1}{\pi_2}}$
    \begin{equation}
        \mathbf{x}^T\mathbf{a} = \langle \mathbf{x}, \mathbf{a} \rangle = \frac{\lVert \mathbf{a} \rVert_2^2}{2} + \sigma^2\log{\frac{\pi_1}{\pi_2}}
    \end{equation}
    
    \begin{itemize}
        \item If unbiased ie. $\pi_1=\pi_2$, then $\mathbf{x}^T\mathbf{a} = \frac{\lVert \mathbf{a} \rVert_2^2}{2}$ which geometrically means the decision hyperplance is in the middle between $\mathbf{0}$ and $\mathbf{a}$ (In 1-D, $x=\frac{a}{2}$).
        \item If biased with no signal ie. $\pi_1>\pi_2$, then $\log{\frac{\pi_1}{\pi_2}}>0$, which geometrically means the decision hyperplance shifts towards $\mathbf{a}$.
        \item If biased with signal ie. $\pi_1<\pi_2$, then $\log{\frac{\pi_1}{\pi_2}}<0$, which geometrically means the decision hyperplance shifts towards $\mathbf{0}$.
    \end{itemize}
    
    \item Prediction (Detection) for some $\mathbf{x} \in \mathbf{R}^d$ based on decision hyperplane:\\
    If $\langle \mathbf{x}, \mathbf{a} \rangle \leq \frac{\lVert \mathbf{a} \rVert_2^2}{2} + \sigma^2\log{\frac{\pi_1}{\pi_2}}$, predict $l=1$ (no signal). Else, predict $l=2$ (signal).
    
\end{enumerate}

\end{itemize}

\subsubsection{Discrete Pixel problem}
\begin{itemize}
    \item Assume molecule with discrete pixels ($mol \in \mathbf{R}^{p \times q}$ and $p>q$ in 2-D), $N$ number of grids, 4 rotations $R \in \{0, \frac{\pi}{2}, \pi, \frac{3\pi}{2}\}$, and $N-p+1$ translations $t \in {0,1,2,...,N-p}$. For each instance (1-D projection) $\mathbf{y} \in \mathbf{R}^N$, each component $y^i$ of $\mathbf{y}$ represents the sum along the ith column. In total, there are $N_c = 4\times(N-p+1)$ configurations (set of translations and rotations) for signals (ie. molecule detected). 
    \item Given molecule $mol$, generate template $A \in \mathbf{R}^{(N-p+1) \times 4 \times N}$ such that $A(i+1,j,:) \in \mathbf{R}^N = a_{t=i, R=(j-1)\frac{\pi}{2}}$ represents a projected signal with certain translation ($i \in {0,..,N-p}$) and rotation ($j \in {1,..,4}$). (We can reshape $A$ into $AA \in \mathbf{R}^{N_c \times N}$ so that each row represents a configuration, ie. a projected signal without noise).
    \item Given an input vector $\mathbf{y} =  \mathbf{a}_{t_0,R_0} + \epsilon_\sigma$ ($\mathbf{y}, \mathbf{a}_{t_0,R_0}, \epsilon_\sigma \in \mathbf{R}^N$ and $ \mathbf{a}_{t,R}$ as signal, $\epsilon_\sigma$ as iid noise $N(0, \sigma^2)$), find most likely $t$, $R$ w.r.t $\mathbf{y}$. ($N_c+1$ possible outcomes with 1 for no match).
    \item Extend to $M$ instances: given an input matrix $\mathbf{Y} = [\mathbf{y_1}^T; \mathbf{y_2}^T; ...; \mathbf{y_M}^T] \in \mathbf{R}^{M\times N}$, for each $\mathbf{y_i}$ find most likely $t$, $R$. (Assume prior $p_0=0.5$ such that half of the instances are just noise, and the rest of instances with signal are evenly distributed into different sets of translation and rotation (configurations) with probability $\frac{1-p_0}{N_c}$)
    \item Methods of detection/prediction (for each $\mathbf{y_i}$, $i \in {1,...,M}$):
    \begin{enumerate}
        \item $\min_{t,R} d_{1}(\by_i,\mathbf{a}_{t,R}) = \min_{t, R} \lVert \by_i- \mathbf{a}_{t,R}\rVert^2$
        \begin{itemize}
            \item This is the same as modeling each configuration of signal($\mathbf{a}_{t,R}$) and the no signal (origin) as a multivariate Gaussian distribution with same noise $\sigma$ (assuming prior for all $N_c+1$ outcomes is equal).
            \item For each $\mathbf{a}_{t,R}$, we can construct a decision hyperplane between $\mathbf{a}_{t,R}$ and $\mathbf{0}$. Choose the nearest hyperplane $\widetilde{b} = \frac{a_{min}}{2}$ between the origin and $\mathbf{a}_{min} = \min{\lVert  \mathbf{a}_{t,R}\rVert}$, we can get a lower bound for false positive rate as $\frac{1}{2} - \frac{1}{2} \erf(\frac{\widetilde{b}}{\sqrt{2}\sigma})$, and a upper bound as $(\frac{1}{2} - \frac{1}{2} \erf(\frac{\widetilde{b}}{\sqrt{2}\sigma}))\cdot N_c$.
        \end{itemize}
        \item $\max_{t,R} d_{2}(\hat{\by_i}, \hat{a_{t,R}}) = \max_{t,R} \langle \hat{\by_i}, \hat{a_{t,R}} \rangle$, where $\hat{v} = \frac{1}{c}(\mathbf{v} - v_m)$ and $v_m = \frac{\sum_{i=1}^N v_i}{N}$, $c = \lVert \mathbf{v}- v_m\rVert$
        \item Let $f(t,\by_i) = \max_R \langle \hat{\by_i}, \hat{a_{t,R}} \rangle$. Pick a threshold $\tau$ based on false positive/negative rates (penalty).\\
        If $f(\widetilde{t},\by_i) > \tau$ and $\widetilde{R} = \argmax_R \langle \hat{\by_i}, \hat{a_{\widetilde{t},R}} \rangle$, then $\widetilde{t}$, $\widetilde{R}$ is a candidate.
        \item Let $g(t,\by_i) = {mean}_R \langle \hat{\by_i}, \hat{a_{t,R}} \rangle$, $h(t,\by_i) = {variance}_R  \langle \hat{\by_i}, \hat{a_{t,R}} \rangle$, and $j(t,\by_i) = \frac{f(t,\by_i)-g(t,\by_i)}{\sqrt{h(t,\by_i)}}$. \\
        If $j(\widetilde{t},\by_i) > \tau$ and $\widetilde{R} = \argmax_R \langle \hat{\by_i}, \hat{a_{\widetilde{t},R}} \rangle$, then $\widetilde{t}$, $\widetilde{R}$ is a candidate.
    \end{enumerate}
    \item Detection/prediction error measurements: given known true labels ($truelabel \in \mathbf{R}^{M\times 2}$, for each $\by_i$, there's a corresponding label $\mathbf{l_i} \in {\{0,..,N-p\}\times{\{1,..,4\}}}$, where $l_i^1$ denotes translation and $l_i^2$ denotes rotation, and the pair $[0,0]^T$ represents no molecule/signal) and predicted labels from one of the above methods ($predlabel \in \mathbf{R}^{M\times 2}$), we can construct a confusion (error) matrix $C$ ($N_c+1$ number of rows and columns):
    
    \begin{tabular}{l|l|l|l|l}
    Truth/Detection&$[0,0]^T$&$[0,1]^T$&...&$[N-p,4]^T$\\\hline
    $[0,0]^T$(No Signal)&Correct& &...& \\\hline
    $[0,1]^T$($t=0,R=0$)& &Correct&...& \\\hline
    ...& & & & \\\hline
    $[N-p,4]^T$($t=N-p,R=\frac{3\pi}{2}$)& & &...&Correct\\\hline
    \end{tabular}
    
    \begin{enumerate}
        \item For each label pair $[i,j]^T$, we can index by $k=i*4+j$, and index $[0,0]^T$ as $k=0$ for noise.
        \item Before normalization, each entry corresponds to the number of occurrences based on truth label and detected label ($\sum_{i}^{N_c+1}\sum_{j}^{N_c+1} C_{ij} = M$).
        \item Normalization: 
        \begin{itemize}
            \item divide each entry by total number of instances ($\sum_{i}^{N_c+1}\sum_{j}^{N_c+1} C_{ij} = 1$).
            \item normalize along each row such that for each row $i \in \{1,...,Nc+1\}$, $\sum_{j}^{N_c+1} C_{ij} = 1$
        \end{itemize}
        \item Perfect detection corresponds to a diagonal matrix (if normalized, $C_{11}=p_0$, $C_{ii}=\frac{1-p_0}{Nc}$ for $i>1$).
        \item We can also consider translation-only error matrix $C_t$. For each label pair, we only care the translation $i \in {\{0,..,N-p\}}$. To keep $i=0$ represents no molecule, we re-index the translation such that $i \in {\{1,..,N-p+1\}}$. So for the error matrix, truth/detection each have $N-p+2$ elements ${\{0,..,N-p+1\}}$.
        \item Observation for $C_t$ (translation-wise): 
            \begin{itemize}
                \item The intersection of the 1st column and 1st row $C_{t}[0,0]=tn$ is the true negative rate (noise detected as noise). 
                \item Regardless of 1st entry, the rest of the first row ($C_{t}[0,i], i \in \{1,...,N-p+1\}$) is roughly the same $a=fp\_noise$, which indicates some false positive rate for the noise (noise detected as signal/molecule).
                \item Regardless of 1st entry, the rest of the first column ($C_{t}[i,0], i \in \{1,...,N-p+1\}$) is roughly the same $b=fn\_noise$, which indicates some false negative rate for the noise (signal/molecule detected as noise).
                \item Regardless of the 1st row and 1st column, the rest of the matrix ($C_{t}[i,j], i,j \in \{1,...,N-p+1\}$) is a Toeplitz matrix. Focus on any row $i$, ie. true label of translation is $i$: 
                \begin{enumerate}
                    \item $C[i,i]$ is the true positive rate for the molecule ($tp_{mol}$) at translation $i$. 
                    \item In an "overlapping" neighborhood of $C[i,i]$, ie. $\{C[i,j]: \lVert j-i \rVert < k\}$ where $k=\max{(p,q)}$, when the predicted signal at the predicted translation $j$ overlaps/touches the true signal at translation $i$, there are some mis-classifying rates ($fc_{j}$, "false class rate" dependent of $j$) for each predicted translation $j \neq i$.
                    \item Outside of the overlapping region, ie. $\{C[i,j]: \lVert j-i \rVert \ge k\}$ where $k=\max{(p,q)}$, there is some similar "noise" rate ($F=fp_{mol}$, "false positive rate for molecule", independent of $j$) for every predicted translation $j$.
                \end{enumerate}
                \item Since the observation above works for any row $i$ (ignore 1st column and 1st row), we can only focus on the 1st and a "central" row of a single true translation. We pick the true translation row $i=\frac{N}{2}$, such that the molecule moves to the middle of the pixels and covers the largest overlapping region. We can recover the full matrix with the first row, first column with all entries the same as the 1st entry of the "central" row, and the rest of entries of other rows can be filled with the overlapping region around the true translation $i$ and noise rates else where. (see table \ref{table:1})
            \begin {table}
            \centering
            \begin{tabular}{l|l|l|l|l|l|l|l}
            Truth/Detection&$0$&...& &$i-1$&$i$&$i+1$&...\\\hline
            $0$(No Signal)&$tn$&... &$a=fp_{noise}$&$a$&$a$&$a$& \\\hline
            ...&$b=fn_{noise}$& & & & & & \\\hline
            $i$&$b$&...&$F=fp_{mol}$ &$fc_{i-1}$&$tp_{mol}$&$fc_{i+1}$&$F$\\\hline
            ...&$b$&...& & & & & \\\hline
            
            \end{tabular}
            \caption{Example of translation-wise error matrix with the "central" row $i=\frac{N_t}{2}$ and molecule of $p=2$ (overlapping region at the ith row is $\{i-1,i+1\}$)}
            \label{table:1}
            \end{table}
            
        \end{itemize}
        \item The above observation can be extended to translation-rotation error matrix $C$. Since the projection for each rotation is the same at any translation, we only care about the different "error rates" from the first row and 4 more rows indicating a single "central" true translation paired up with 4 true rotations. (ie. row $[0, 0]^T, [\frac{N}{2}, 1]^T, [\frac{N}{2}, 2]^T, [\frac{N}{2}, 3]^T, [\frac{N}{2}, 4]^T$)
        \item If we use the threshold prediction/detection instead of min/max "distance", each instance may get multiple predicted/detected labels. In this case, we consider each prediction of each instance as an occurrence to add in the error matrix. (We no longer normalize the matrix by number of instances $M$, since there are in total more than $M$ occurrences.)
        
        
    \end{enumerate}
    
\end{itemize}

\subsubsection{Histogram study}
Let $\epsilon_{\sigma}^{i}  \in \mathbb{R}^{N}$, $i=1,2,\ldots M$, denote iid noise vectors with in $\mathbb{R}^{N}$ with mean $0$ and variance $\sigma^2$ as before. 

\begin{enumerate}
\item Plot the histogram of values $d_{1}(\epsilon_{\sigma}^i, \ba_{t,R}), d_{2}(\epsilon_{\sigma}^i, \ba_{t,R})$, $t\in 1,2,\ldots N-p$, $R \in \{ 0,\pi/2, \pi, 3\pi/2 \}$
\item For fixed R, plot the histogram values $d_{1}(\epsilon_{\sigma}^i, \ba_{t,R}), d_{2}(\epsilon_{\sigma}^i, \ba_{t,R})$, $t\in 1,2,\ldots N-p$. Also plot the histogram values of $f(t,\epsilon_{\sigma}^i)$, and $j(t,\epsilon_{\sigma}^i)$.
\end{enumerate}


\subsection{\date{Jun 17th}}
LDA
\begin{itemize}
    \item Determine decision boundary (in any dimension d, $\mathbf{x} \in \mathbf{R}^d$) with prior and penalty matrix. (two classes with $\mathbf{\mu}_1 = \mathbf{0}$ (no signal) and $\mathbf{\mu}_2 = \mathbf{a}$ (signal))
\begin{enumerate}
    \item Assume prior (number of occurrences) for no signal is $\pi_1$, and prior for signal is $\pi_2$ ($\pi_2 = 1-\pi_1$). 
    \item Penalty/Cost Matrix (corresponding to the Error Matrix):
    
    \begin{tabular}{l|l|l}
    Truth/Detection&No Signal&Signal\\\hline
    No Signal&0&F (fp)\\\hline
    Signal&M (fn)&0\\
    \end{tabular}
    \item Decision Rule: If $\langle \mathbf{x}, \frac{\mathbf{a} }{\lVert \mathbf{a} \rVert}\rangle \le b$, predict $l=1$ (no signal). Else, predict $l=2$ (signal). $b$ is the decision point.
    \item False positive rate and false negative rate given decision point $b$:
    \begin{equation}
        fp = \alpha(b) = \int_{\frac{b}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(\frac{b}{\sqrt{2}\sigma})
    \end{equation}
    
    \begin{equation}
        fn= \beta(b) = \int_{-\infty}^{\frac{b-a}{\sigma}} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \int_{-\frac{b-a}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(-\frac{b-\lVert \mathbf{a} \rVert}{\sqrt{2}\sigma})
    \end{equation}
    
    \item Expected cost function: $\mathbf{E}[J] = \pi_1\alpha(b)F+(1-\pi_1)\beta(b)M$
    \item Minimize cost function w.r.t. $b$, assuming $M=1$.\\
    $\min_b \mathbf{E}[J]$, Take $\nabla \mathbf{E}[J] = 0$\\\\
    $\pi_1 F(-\frac{1}{2}\cdot\frac{2}{\sqrt{\pi}}\cdot e^{-\frac{b^2}{2\sigma^2}}\cdot \frac{1}{\sqrt{2}\sigma}) +
    (1-\pi_1)(-\frac{1}{2}\cdot\frac{2}{\sqrt{\pi}}\cdot e^{-\frac{{(b-\lVert \mathbf{a} \rVert)}^2}{2\sigma^2}}\cdot -\frac{1}{\sqrt{2}\sigma})=0$\\\\
    $\frac{1-\pi_1}{\sqrt{2\pi}\sigma} e^{-\frac{{(b-\lVert \mathbf{a} \rVert)}^2}{2\sigma^2}} = \frac{\pi_1 F}{\sqrt{2\pi}\sigma} e^{-\frac{b^2}{2\sigma^2}}$\\\\
    $\log{(1-\pi_1)}-\frac{{(b-\lVert \mathbf{a} \rVert)}^2}{2\sigma^2} =
    \log{(\pi_1F)}-\frac{b^2}{2\sigma^2}$\\\\
    $\frac{b^2-{(b-\lVert \mathbf{a} \rVert)}^2}{2\sigma^2} = log{(\frac{\pi_1F}{1-\pi_1})}$\\\\
    $2\lVert \mathbf{a} \rVert b - {\lVert \mathbf{a} \rVert}^2 = 2\sigma^2log{(\frac{\pi_1F}{1-\pi_1})}$\\\\
    \begin{equation}
        b = \frac{\sigma^2log{(\frac{\pi_1F}{1-\pi_1})}}{\lVert \mathbf{a} \rVert} + \frac{\lVert \mathbf{a} \rVert}{2}
    \end{equation}
    
\end{enumerate}
    
\end{itemize}


\end{document}