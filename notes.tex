\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{amsmath}

\title{Summer 2021}
\author{Tanya}
\date{June 2021}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{Introduction}
This project is about... 


\section{Notes}

\subsection{\date{Jun 10th}}
LDA (Linear Discriminant Analysis, $\vec{x}$ as input features, $y$ as output label): 
\begin{itemize}
    \item 1-D ($x \in R$) false positive and false negative rates (no prior or unbiased):
\begin{enumerate}
    \item Model Gaussian PDF for each class $k \in \{1,..,K\}$:
    $p(x|y=k) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{{(x-\mu_k)}^2}{2\sigma^2}}$,\\
    given $\sigma^2=\frac{\sum_{i=1}^{i=N} {(x_i-\mu_i)}^2}{N}$ and $\mu_k = \frac{\sum_{i=1}^{i=N_k} x_i}{N_k}$ from N training data $x_i$ with labels. 
    \item Assume two classes with $\mu_1 = 0$ (no signal) and $\mu_2 = a$ (signal) with same noise $\sigma$. Find decision boundary between the 2 classes to be $x=\frac{a}{2}$ where $p(x=\frac{a}{2}|y=1) = p(x=\frac{a}{2}|y=2)$ (Maximum likelihood classifier).
    \item Detection (prediction) of some $x$: \\
    Maximize likelihood: if $p(x|y=1) >= p(x|y=2)$, predict $y=1$ (no signal), else $y=2$ (signal).\\
    Decision boundary (geometric): if $x <= \frac{a}{2}$, predict $y=1$ (no signal), else $y=2$ (signal).
    \item Error Matrix:
    
    \begin{tabular}{l|l|l}
    Truth/Detection&No Signal&Signal\\\hline
    No Signal&Correct&False Positive\\\hline
    Signal&False Negative&Correct\\
    \end{tabular}
    \item False Positive rate: the area of the right tail of label-1 (no signal) Gaussian PDF beyond decision boundary $x=b$.\\
    False Positive rate $ = \int_{\frac{b}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} erf(\frac{b}{\sqrt{2}\sigma})$.\\
    False Negative rate: the area of the left tail of label-2 (signal) Gaussian PDF below decision boundary $x=b$.\\
    False Positive rate $ = \int_{-\infty}^{\frac{b-a}{\sigma}} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \int_{-\frac{b-a}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} erf(-\frac{b-a}{\sqrt{2}\sigma})$.
    
    \item Since there is no prior (unbiased) and decision boundary $b=\frac{a}{2}$, False Negative rate $=$ False Positive rate $= \frac{1}{2} - \frac{1}{2} erf(\frac{a}{2\sqrt{2}\sigma})$.
\end{enumerate}

\item Determine decision boundary (geometrically in any dimension d, $\vec{x} \in R^d$) with prior:
\begin{enumerate}
    \item Model multivariate Gaussian PDF for each class $k \in \{1,..,K\}$:\\
    $p(\vec{x}|y=k) = \frac{1}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\vec{x}-\vec{\mu}_k)}^T \Sigma^{-1}{(\vec{x}-\vec{\mu}_k)}}$,\\
    given $\Sigma$ is the (within-class) covariance matrix and $\vec{\mu}_k = \frac{\sum_{i=1}^{i=N_k} \vec{x}_i}{N_k}$ from N training data $\vec{x}_i$ with labels. 
    \item Apply Bayesian Theorem to get posterior probability: $p(\vec{x}, y=k) = p(\vec{x}|y=k) \cdot p(y=k)$, given the prior $p(y=k) = \pi_k$.
    \item Assume two classes with $\vec{\mu}_1 = \vec{0}$ (no signal) and $\vec{\mu}_2 = \vec{a}$ (signal $\vec{a} \in R^d$) with same covariance $\Sigma = \sigma^2 I$ (given noise $\sigma$ for each independent feature). Find decision boundary (hyperplane) between the 2 classes such that $p(\vec{x}|y=1)\cdot p(y=1) = p(\vec{x}|y=2)\cdot p(y=2)$. (Note: $\pi_1 = p(\vec{0})$, $\pi_2 = p(\vec{a})$)\\
    $\frac{\pi_1}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\vec{x}-\vec{\mu}_1)}^T \Sigma^{-1}{(\vec{x}-\vec{\mu}_1)}} = \frac{\pi_2}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\vec{x}-\vec{\mu}_2)}^T \Sigma^{-1}{(\vec{x}-\vec{\mu}_2)}}$ \\\\
    $2\log{\frac{\pi_1}{\pi_2}} = {(\vec{x}-\vec{\mu}_1)}^T \Sigma^{-1}{(\vec{x}-\vec{\mu}_1)} - {(\vec{x}-\vec{\mu}_2)}^T \Sigma^{-1}{(\vec{x}-\vec{\mu}_2)}$\\\\
    $\vec{x}^T\vec{x} - {(\vec{x}-\vec{a})}^T(\vec{x}-\vec{a}) = 2\sigma^2\log{\frac{\pi_1}{\pi_2}}$\\\\
    $\vec{x}^T\vec{a} = <\vec{x}, \vec{a}> = \frac{\lVert \vec{a} \rVert_2^2}{2} + \sigma^2\log{\frac{\pi_1}{\pi_2}}$
    \begin{itemize}
        \item If unbiased ie. $\pi_1=\pi_2$, then $\vec{x}^T\vec{a} = \frac{\lVert \vec{a} \rVert_2^2}{2}$ which geometrically means the decision hyperplance is in the middle between $\vec{0}$ and $\vec{a}$ (In 1-D, $x=\frac{a}{2}$).
        \item If biased with no signal ie. $\pi_1>\pi_2$, then $\log{\frac{\pi_1}{\pi_2}}>0$, which geometrically means the decision hyperplance shifts towards $\vec{a}$.
        \item If biased with signal ie. $\pi_1<\pi_2$, then $\log{\frac{\pi_1}{\pi_2}}<0$, which geometrically means the decision hyperplance shifts towards $\vec{0}$.
    \end{itemize}
    
    \item Prediction (Detection) for some $\vec{x} \in R^d$ based on decision hyperplane: If $<\vec{x}, \vec{a}> <= \frac{\lVert \vec{a} \rVert_2^2}{2} + \sigma^2\log{\frac{\pi_1}{\pi_2}}$, predict $y=1$ (no signal). Else, predict $y=2$ (signal).
    
\end{enumerate}



\end{itemize}


\end{document}