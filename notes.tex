\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{amsmath,amssymb}

\title{Summer 2021}
\author{Tanya}
\date{June 2021}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\DeclareMathOperator{\erf}{erf}

\begin{document}

\maketitle


\section{LDA (Linear Discriminant Analysis)}
\begin{itemize}
    \item Notations: $\mathbf{x}$ as input features, $l$ as output label
\end{itemize}

\subsection{Decision Boundary with Prior}

\begin{enumerate}
    \item Model multivariate Gaussian PDF for each class $k \in \{1,..,K\}$:\\
    $p(\mathbf{x}|l=k) = \frac{1}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\mathbf{x}-\mathbf{\mu_k})}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu_k})}}$, $\mathbf{x} \in \mathbf{R}^d$\\
    given $\Sigma$ is the (within-class) covariance matrix and $\mathbf{\mu_k} = \frac{\sum_{i=1}^{i=N_k} \mathbf{x}_i}{N_k}$ from N training data $\mathbf{x}_i$ with labels. 
    
    \item Apply Bayesian Theorem to get posterior probability: $p(\mathbf{x}, l=k) = p(\mathbf{x}|l=k) \cdot p(l=k)$, given the prior $p(l=k) = \pi_k$.
    
    \item Assume two classes with $\mathbf{\mu}_1 = \mathbf{0}$ (no signal) and $\mathbf{\mu}_2 = \mathbf{a}$ (signal $\mathbf{a} \in \mathbf{R}^d$) with same covariance $\Sigma = \sigma^2 I$ (given noise $\sigma$ for each independent feature). Find decision boundary (hyperplane) between the 2 classes such that $p(\mathbf{x}|l=1)\cdot p(l=1) = p(\mathbf{x}|l=2)\cdot p(l=2)$. (Note: $\pi_1 = p(\mathbf{0})$, $\pi_2 = p(\mathbf{a})$).
    This is called the MAP (max a posterior) decision rule. Let's derive the decision boundary:
    $\frac{\pi_1}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\mathbf{x}-\mathbf{\mu}_1)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_1)}} = \frac{\pi_2}{{(2\pi)}^{\frac{d}{2}} {(det\Sigma)}^{\frac{1}{2}}} e^{-\frac{1}{2}{(\mathbf{x}-\mathbf{\mu}_2)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_2)}}$ \\\\
    $2\log{\frac{\pi_1}{\pi_2}} = {(\mathbf{x}-\mathbf{\mu}_1)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_1)} - {(\mathbf{x}-\mathbf{\mu}_2)}^T \Sigma^{-1}{(\mathbf{x}-\mathbf{\mu}_2)}$\\\\
    $2\log{\frac{\pi_1}{\pi_2}} = \frac{1}{\sigma^2}{(\mathbf{x}-\mathbf{\mu}_1)}^T I^{-1}{(\mathbf{x}-\mathbf{\mu}_1)} - \frac{1}{\sigma^2}{(\mathbf{x}-\mathbf{\mu}_2)}^T I^{-1}{(\mathbf{x}-\mathbf{\mu}_2)}$ (replace $\Sigma$ with $\sigma^2 I$)\\\\
    $\mathbf{x}^T\mathbf{x} - {(\mathbf{x}-\mathbf{a})}^T(\mathbf{x}-\mathbf{a}) = 2\sigma^2\log{\frac{\pi_1}{\pi_2}}$
    \begin{equation}
        \frac{\mathbf{x}^T\mathbf{a}}{\lVert \mathbf{a} \rVert} = \frac{\langle \mathbf{x}, \mathbf{a} \rangle}{\lVert \mathbf{a} \rVert} = \frac{\lVert \mathbf{a} \rVert}{2} + \frac{\sigma^2}{\lVert \mathbf{a} \rVert}\log{\frac{\pi_1}{\pi_2}}
    \end{equation}
    
    \begin{itemize}
        \item If unbiased ie. $\pi_1=\pi_2$, then $\frac{\mathbf{x}^T\mathbf{a}}{\lVert \mathbf{a} \rVert} = \frac{\lVert \mathbf{a} \rVert}{2}$ which geometrically means the decision hyperplance is in the middle between $\mathbf{0}$ and $\mathbf{a}$ (Projected onto 1-D, the decision point is $x=\frac{\lVert \mathbf{a} \rVert}{2}$).
        \item If biased with no signal ie. $\pi_1>\pi_2$, then $\log{\frac{\pi_1}{\pi_2}}>0$, which geometrically means the decision hyperplance shifts towards $\mathbf{a}$.
        \item If biased with signal ie. $\pi_1<\pi_2$, then $\log{\frac{\pi_1}{\pi_2}}<0$, which geometrically means the decision hyperplance shifts towards $\mathbf{0}$.
    \end{itemize}
    
    \item Prediction (Detection) for some $\mathbf{x} \in \mathbf{R}^d$ based on decision hyperplane:\\
    If $\langle \mathbf{x}, \mathbf{a} \rangle \leq \frac{\lVert \mathbf{a} \rVert_2^2}{2} + \sigma^2\log{\frac{\pi_1}{\pi_2}}$, predict $l=1$ (no signal). Else, predict $l=2$ (signal).
    
\end{enumerate}

\subsection{False Positive and False Negative Rates in/projected-onto 1-D}
    
    \begin{enumerate}
    \item Model Gaussian PDF for each class $k \in \{1,..,K\}$:
    $p(x|l=k) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{{(x-\mu_k)}^2}{2\sigma^2}}$ ($x \in \mathbf{R}$),\\
    given $\sigma^2=\frac{\sum_{i=1}^{i=N} {(x_i-\mu_i)}^2}{N}$ and $\mu_k = \frac{\sum_{i=1}^{i=N_k} x_i}{N_k}$ from N training data $x_i$ with labels. 
    \item Assume two classes with $\mu_1 = 0$ (no signal) and $\mu_2 = a$ (signal) with same noise $\sigma$. Find decision boundary between the 2 classes to be $x=\frac{a}{2}$ where $p(x=\frac{a}{2}|l=1) = p(x=\frac{a}{2}|l=2)$ (Maximum likelihood classifier).
    \item Detection (prediction) of some $x$: \\
    Maximize likelihood: if $p(x|l=1) \ge p(x|l=2)$, predict $l=1$ (no signal), else $l=2$ (signal).\\
    Decision boundary (geometric): if $x \leq \frac{a}{2}$, predict $l=1$ (no signal), else $l=2$ (signal).
    \item Error Matrix:
    
    \begin{tabular}{l|l|l}
    Truth/Detection&No Signal&Signal\\\hline
    No Signal&Correct&False Positive\\\hline
    Signal&False Negative&Correct\\
    \end{tabular}
    \item False Positive rate: the area of the right tail of label-1 (no signal) Gaussian PDF beyond decision boundary $x=b$.
    \begin{equation}
        fp = \int_{\frac{b}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(\frac{b}{\sqrt{2}\sigma})
    \label{eq:1}
    \end{equation}
    
    False Negative rate: the area of the left tail of label-2 (signal) Gaussian PDF below decision boundary $x=b$.
    
    \begin{equation}
        fn = \int_{-\infty}^{\frac{b-a}{\sigma}} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \int_{-\frac{b-a}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(-\frac{b-a}{\sqrt{2}\sigma})
    \label{eq:2}
    \end{equation}
    
    
    \item Since there is no prior (unbiased) and decision boundary $b=\frac{a}{2}$, False Negative rate $=$ False Positive rate $= \frac{1}{2} - \frac{1}{2} \erf(\frac{a}{2\sqrt{2}\sigma})$.
\end{enumerate}

\subsection{Decision Boundary with Prior and Penalty Matrix}

\begin{enumerate}
    \item Determine decision boundary (in any dimension d, $\mathbf{x} \in \mathbf{R}^d$) with prior and penalty matrix. (two classes with $\mathbf{\mu}_1 = \mathbf{0}$ (no signal) and $\mathbf{\mu}_2 = \mathbf{a}$ (signal))
    \item Assume prior (number of occurrences) for no signal is $\pi_1$, and prior for signal is $\pi_2$ ($\pi_2 = 1-\pi_1$). 
    \item Penalty/Cost Matrix (corresponding to the Error Matrix):
    
    \begin{tabular}{l|l|l}
    Truth/Detection&No Signal&Signal\\\hline
    No Signal&0&F (fp)\\\hline
    Signal&M (fn)&0\\
    \end{tabular}
    \item Decision Rule: If $\langle \mathbf{x}, \frac{\mathbf{a} }{\lVert \mathbf{a} \rVert}\rangle \le b$, predict $l=1$ (no signal). Else, predict $l=2$ (signal). $b$ is the decision point.
    \item False positive rate and false negative rate given decision point $b$:
    \begin{equation}
        fp = \alpha(b) = \int_{\frac{b}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(\frac{b}{\sqrt{2}\sigma})
    \end{equation}
    
    \begin{equation}
        fn= \beta(b) = \int_{-\infty}^{\frac{b-a}{\sigma}} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \int_{-\frac{b-a}{\sigma}}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt = \frac{1}{2} - \frac{1}{2} \erf(-\frac{b-\lVert \mathbf{a} \rVert}{\sqrt{2}\sigma})
    \end{equation}
    
    \item Expected cost function: $\mathbf{E}[J] = \pi_1\alpha(b)F+(1-\pi_1)\beta(b)M$
    \item Minimize cost function w.r.t. $b$, assuming $M=1$.\\
    $\min_b \mathbf{E}[J]$, Take $\nabla \mathbf{E}[J] = 0$\\\\
    $\pi_1 F(-\frac{1}{2}\cdot\frac{2}{\sqrt{\pi}}\cdot e^{-\frac{b^2}{2\sigma^2}}\cdot \frac{1}{\sqrt{2}\sigma}) +
    (1-\pi_1)(-\frac{1}{2}\cdot\frac{2}{\sqrt{\pi}}\cdot e^{-\frac{{(b-\lVert \mathbf{a} \rVert)}^2}{2\sigma^2}}\cdot -\frac{1}{\sqrt{2}\sigma})=0$\\\\
    $\frac{1-\pi_1}{\sqrt{2\pi}\sigma} e^{-\frac{{(b-\lVert \mathbf{a} \rVert)}^2}{2\sigma^2}} = \frac{\pi_1 F}{\sqrt{2\pi}\sigma} e^{-\frac{b^2}{2\sigma^2}}$\\\\
    $\log{(1-\pi_1)}-\frac{{(b-\lVert \mathbf{a} \rVert)}^2}{2\sigma^2} =
    \log{(\pi_1F)}-\frac{b^2}{2\sigma^2}$\\\\
    $\frac{b^2-{(b-\lVert \mathbf{a} \rVert)}^2}{2\sigma^2} = log{(\frac{\pi_1F}{1-\pi_1})}$\\\\
    $2\lVert \mathbf{a} \rVert b - {\lVert \mathbf{a} \rVert}^2 = 2\sigma^2log{(\frac{\pi_1F}{1-\pi_1})}$\\\\
    \begin{equation}
        b = \frac{\sigma^2log{(\frac{\pi_1F}{1-\pi_1})}}{\lVert \mathbf{a} \rVert} + \frac{\lVert \mathbf{a} \rVert}{2}
    \end{equation}
    
\end{enumerate}


\section{Discrete Pixel Problem}

\subsection{Set-up}
\begin{itemize}
    \item Molecule: Assume there's some molecule $m \in \mathbf{R}^{p \times q}$ ($p\ge q$) in 2-D, with 1-D projection onto $N$ number of pixels. The molecule $m$ has $N-p+1$ translations $t \in \{0,1,2,...,N-p\}$, and 4 rotations $R \in \{0, \frac{\pi}{2}, \pi, \frac{3\pi}{2}\}$ at every discrete position. For each instance (1-D projection) $\mathbf{y} \in \mathbf{R}^N$, each component $y^i$ of $\mathbf{y}$ represents the sum along the ith column. In total, there are $N_c = 4\times(N-p+1)$ configurations (set of translations and rotations) for signals (ie. molecule detected). 
    \item Template: Given a molecule $m$, we can generate a set of templates $A \in \mathbf{R}^{N_c \times N}$ such that each row $A(k,:) \in \mathbf{R}^N = a_{t=i, R=(j-1)\frac{\pi}{2}}$ represents a projected signal with certain translation ($i \in {0,..,N-p}$) and rotation ($j \in {1,..,4}$). (We index each unique pair of translation $i$ and rotation $j$ as a positive class $k=i+4\cdot j$, and $0$ represents the no molecule case.)
    \item Random (input) data with noise:
    \begin{enumerate}
        \item Definition of an input vector $\mathbf{y}$: $\mathbf{y} =  \mathbf{a}_{t_0,R_0} + \epsilon_\sigma$ ($\mathbf{y}, \mathbf{a}_{t_0,R_0}, \epsilon_\sigma \in \mathbf{R}^N$ and $ \mathbf{a}_{t_0,R_0}$ as some signal, $\epsilon_\sigma$ as iid noise $N(0, \sigma^2)$)
        \item Extend to $M$ instances: an input matrix $\mathbf{Y} = [\mathbf{y_1}^T; \mathbf{y_2}^T; ...; \mathbf{y_M}^T] \in \mathbf{R}^{M\times N}$
    \end{enumerate}
    \end{itemize}
    
    \subsection{Detection/Prediction for random data}
    \begin{itemize}
        \item Given an input matrix $\mathbf{Y} = [\mathbf{y_1}^T; \mathbf{y_2}^T; ...; \mathbf{y_M}^T] \in \mathbf{R}^{M\times N}$, for each $\mathbf{y_i}$ find most likely $t$, $R$ w.r.t $\mathbf{y_i}$ ($N_c+1$ possible outcomes with 1 for no molecule). (Assume prior $p_0=0.5$ such that half of the instances are just noise, and the rest of instances with signal are evenly distributed into different sets of translation and rotation (configurations) with probability $\frac{1-p_0}{N_c}$)
        \item Methods of detection/prediction (for each $\mathbf{y_i}$, $i \in {1,...,M}$)
        \begin{enumerate}
        \item $\min_{t,R} d_{1}(\by_i,\mathbf{a}_{t,R}) = \min_{t, R} \lVert \by_i- \mathbf{a}_{t,R}\rVert^2$
        \begin{itemize}
            \item This is the same as modeling each configuration of signal($\mathbf{a}_{t,R}$) and the no signal (origin) as a multivariate Gaussian distribution with same noise $\sigma$ (assuming prior for all $N_c+1$ outcomes is equal).
            \item Theoretical Bounds for false positive/negative rates: For each $\mathbf{a}_{t,R}$, we can construct a decision hyperplane between $\mathbf{a}_{t,R}$ and $\mathbf{0}$. Choose the nearest hyperplane $\widetilde{b} = \frac{a_{min}}{2}$ between the origin and $\mathbf{a}_{min} = \min{\lVert  \mathbf{a}_{t,R}\rVert}$, we can get a lower bound for false positive rate as $\frac{1}{2} - \frac{1}{2} \erf(\frac{\widetilde{b}}{\sqrt{2}\sigma})$, and a upper bound as $(\frac{1}{2} - \frac{1}{2} \erf(\frac{\widetilde{b}}{\sqrt{2}\sigma}))\cdot N_c$.
        \end{itemize}
        \item $\max_{t,R} d_{2}(\hat{\by_i}, \hat{\ba_{t,R}}) = \max_{t,R} \langle \hat{\by_i}, \hat{\ba_{t,R}} \rangle$, where $\hat{v} = \frac{1}{c}(\mathbf{v} - v_m)$ and $v_m = \frac{\sum_{i=1}^N v_i}{N}$, $c = \lVert \mathbf{v}- v_m\rVert$
        \begin{itemize}
            \item Since the noise center (origin) $\mathbf{0}$ cannot be "normalized" and always $d_{2}(\hat{\by_i}, \mathbf{0})=0$, the false positive rate could be really high (almost $1$) by maximizing $d_2$.
            \item To make this method reasonably separate positive (signal) class and negative (noise) class, we add a threshold $\tau$ to make the decision. If $\max_{t,R} d_{2}(\hat{\by_i}, \hat{\ba_{t,R}}) = \max_{t,R} \langle \hat{\by_i}, \hat{\ba_{t,R}} \rangle \ge \tau$, we choose the corresponding maximizer $\widetilde{t}$, $\widetilde{R}$ as the predicted positive label; else, we assign label $0$ (no molecule).
            \item We can choose the threshold $\tau$ based on histogram study or empirical error rates for the prediction.
        \end{itemize}
        \item Let $f(t,\by_i) = \max_R \langle \hat{\by_i}, \hat{\ba_{t,R}} \rangle$. Pick a threshold $\tau$ based on false positive/negative rates (penalty).\\
        If $f(\widetilde{t},\by_i) > \tau$ and $\widetilde{R} = \argmax_R \langle \hat{\by_i}, \hat{\ba_{\widetilde{t},R}} \rangle$, then $\widetilde{t}$, $\widetilde{R}$ is a candidate.
        \item Let $g(t,\by_i) = {mean}_R \langle \hat{\by_i}, \hat{\ba_{t,R}} \rangle$, $h(t,\by_i) = {variance}_R  \langle \hat{\by_i}, \hat{a_{t,R}} \rangle$, and $j(t,\by_i) = \frac{f(t,\by_i)-g(t,\by_i)}{\sqrt{h(t,\by_i)}}$. \\
        If $j(\widetilde{t},\by_i) > \tau$ and $\widetilde{R} = \argmax_R \langle \hat{\by_i}, \hat{\ba_{\widetilde{t},R}} \rangle$, then $\widetilde{t}$, $\widetilde{R}$ is a candidate.
    \end{enumerate}
    
    \end{itemize}
    
    \subsection{Detection/Prediction Error Measurements}
    
    \subsubsection{Confusion/Error matrix}
    Given known true labels ($\mathbf{L_T} \in \mathbf{R}^{M\times 1}$) and predicted labels ($\mathbf{L_P} \in \mathbf{R}^{M\times 1}$), we can construct a confusion (error) matrix $C$ ($N_c+1$ number of rows and columns):
    \\\\
    
    \begin{tabular}{l|l|l|l|l}
        Truth/Detection&$0$&$1$&...&$N_c$\\\hline
        $0$ (No Signal)&Correct& &...& \\\hline
        $1$ ($t=0,R=0$)& &Correct&...& \\\hline
        ...& & & & \\\hline
        $N_c$ ($t=N-p,R=\frac{3\pi}{2}$)& & &...&Correct\\\hline
    \end{tabular}
        
        \begin{enumerate}
            \item For each instance $\by_k$ ($k \in \{1,...,M\}$), there's a corresponding true label $\mathbf{L_T}[k] \in {\{0,..,N_c\}} = i \cdot 4 + j$, indicating a unique pair of translation $i \in {\{0,..,N-p\}}$ and rotation $j \in {\{1,..,4\}}$), and similarly a corresponding predicted label $\mathbf{L_P}[k]$ (detected from one of the above methods). We can add up the number of occurrences to the corresponding entries in the confusion matrix $C$.
            \item $0$ represents the no molecule (noise) case and corresponds to translation $0$ and rotation $0$. (We always put the noise label $0$ as the first class)
            \item Before normalization, each entry corresponds to the number of occurrences based on truth label and detected label ($\sum_{i=1}^{N_c+1}\sum_{j=1}^{N_c+1} C_{ij} = M$).
            \item Normalization: 
            \begin{itemize}
                \item Divide each entry by total number of instances ($\sum_{i=1}^{N_c+1}\sum_{j=1}^{N_c+1} C_{ij} = 1$).
                \item Normalize along each row such that for each row $i \in \{1,...,Nc+1\}$, $\sum_{j=1}^{N_c+1} C_{ij} = 1$. (This means each ith row gives the fractions of classifications given to the true label $i$.)
            \end{itemize}
            \item Perfect detection corresponds to a diagonal matrix (if normalized, $C_{11}=p_0$, $C_{ii}=\frac{1-p_0}{Nc}$ for $i>1$).
            \item If we use the threshold prediction/detection for each translation instead of min/max "distance" over all pairs of translation and rotation, each instance may get multiple predicted/detected labels. In this case, we consider each prediction of each instance as an occurrence to add in the error matrix. 
        \end{enumerate}
        
    \subsubsection{Translation-wise Confusion Matrix}
     We can also construct a Translation-wise Confusion Matrix $C_t$ ($N_t+1=N-p+2$ number of rows and columns):
     \\\\
        \begin{tabular}{l|l|l|l|l}
        Truth/Detection&$0$&$1$&...&$N_t=N-p$\\\hline
        $0$ (No Signal)&Correct& &...& \\\hline
        $1$ ($t=0$)& &Correct&...& \\\hline
        ...& & & & \\\hline
        $N_t$ ($t=N-p$)& & &...&Correct\\\hline
        \end{tabular}
        
        \begin{enumerate}
            \item For each instance $\by_k$ ($k \in \{1,...,M\}$), we only care about the true translation label $\mathbf{\tilde{L}_T}[k] = \floor{\frac{\mathbf{L_T}[k]}{4}} = i \in {\{0,..,N-p\}}$ and the predicted label $\mathbf{\tilde{L}_P}[k]$. To keep $i=0$ represents no molecule, we re-index the translation such that $i \in {\{1,..,N-p+1\}}$. So for the error matrix, truth/detection each have $N-p+2$ elements ${\{0,..,N-p+1\}}$.
            
            \item The intersection of the 1st column and 1st row $C_{t}[0,0]=tn$ is the true negative rate $c$ (noise detected as noise). 
                \item Regardless of 1st entry, the rest of the first row ($C_{t}[0,i], i \in \{1,...,N-p+1\}$) is roughly the same value $a$, which indicates some false positive rate for the noise (noise detected as signal/molecule).
                \item Regardless of 1st entry, the rest of the first column ($C_{t}[i,0], i \in \{1,...,N-p+1\}$) is roughly the same value $b$, which indicates some false negative rate for the noise (signal/molecule detected as noise).
                \item Regardless of the 1st row and 1st column, the rest of the matrix ($C_{t}[i,j], i,j \in \{1,...,N-p+1\}$) is a Toeplitz matrix. Focus on any row $i$, ie. true label of translation is $i$: 
                \begin{enumerate}
                    \item $d=C[i,i]$ is the true positive rate for the molecule at translation $i$. 
                    \item In an "overlapping" neighborhood of $C[i,i]$, ie. $\{C[i,j]: \lVert j-i \rVert < k\}$ where $k=\max{(p,q)}$, when the predicted signal at the predicted translation $j$ overlaps/touches the true signal at translation $i$, there are some mis-classifying rates $e_{z}=C[i,i+z]$ ("false class rate" dependent of $z=j-i$) for each predicted translation $j \neq i$.
                    \item Outside of the overlapping region, ie. $\{C[i,j]: \lVert j-i \rVert \ge k\}$ where $k=\max{(p,q)}$, there is some similar "noise" rate $F=C[i,j]$ ("false positive rate for molecule", independent of $j$) for every predicted translation $j$.
                \end{enumerate}
            
            \item Since the observation above works for any row $i$ (ignore 1st column and 1st row), we can only focus on the 1st and a "central" row of a single true translation. We pick the true translation $i=\ceil{\frac{N}{2}}$, such that the molecule moves to the middle of the pixels and covers the largest overlapping region. We can recover the full matrix with the first row, first column with all entries the same as the 1st entry of the "central" row, and the rest of entries of other rows can be filled with the overlapping region around the true translation $i$ and noise rates else where. (An example of "reduced" translation-wise error matrix with the "central" row $i=\ceil{\frac{N_t}{2}}$ and molecule of $p=2$ (overlapping region at the ith row is $\{i-1,i+1\}$))
            \\\\
            \begin{tabular}{l|l|l|l|l|l|l|l}
            Truth/Detection&$0$&...& &$i-1$&$i$&$i+1$&...\\\hline
            $0$(No Signal)&$c$&... &$a$&$a$&$a$&$a$& \\\hline
            ...&$b$& & & & & & \\\hline
            $i$&$b$&...&$F$ &$e_{-1}$&$d$&$e_{1}$&$F$\\\hline
            ...&$b$&...& & & & & \\\hline
            
            \end{tabular}
            
            \item The above observation can be extended to translation-rotation error matrix $C$. Since the projection for each rotation is the same at any translation, we only care about the different "error rates" from the first row and 4 more rows indicating a single "central" true translation paired up with 4 true rotations. \\ (ie. For "reduced" $C$, there are rows $[0, 0]^T, [\ceil{\frac{N}{2}}, 1]^T, [\ceil{\frac{N}{2}}, 2]^T, [\ceil{\frac{N}{2}}, 3]^T, [\ceil{\frac{N}{2}}, 4]^T$)
            
        \end{enumerate}
        

\subsection{Histogram study}
Let $\epsilon_{\sigma}^{i}  \in \mathbb{R}^{N}$, $i=1,2,\ldots M$, denote iid noise vectors with in $\mathbb{R}^{N}$ with mean $0$ and variance $\sigma^2$ as before. (Based on the the approximate distribution of $d(\epsilon_{\sigma}^i, \ba_{t,R})$ and a threshold $\tau$, we can preview the approximate false positive rate by adding up the relative-frequencies for $d>\tau$.)

\begin{enumerate}
\item Plot the histogram of values $d_{1}(\epsilon_{\sigma}^i, \ba_{t,R}), d_{2}(\epsilon_{\sigma}^i, \ba_{t,R})$, $t\in 1,2,\ldots N-p$, $R \in \{ 0,\pi/2, \pi, 3\pi/2 \}$ 
\item For fixed R, plot the histogram values $d_{1}(\epsilon_{\sigma}^i, \ba_{t,R}), d_{2}(\epsilon_{\sigma}^i, \ba_{t,R})$, $t\in 1,2,\ldots N-p$. Also plot the histogram values of $f(t,\epsilon_{\sigma}^i)$, and $j(t,\epsilon_{\sigma}^i)$.
\end{enumerate}




\end{document}